#!/bin/bash
# To use sparkR-submit, we assume the SparkR package in yarn-cluster mode
# we assume that it has been installed to a standard location using
# R CMD INSTALL pkg/

FWDIR="$(cd `dirname $0`; pwd)"

export PROJECT_HOME="$FWDIR"

export SPARKR_JAR_FILE="$FWDIR/lib/SparkR/sparkr-assembly-0.1.jar"

# Exit if the user hasn't set SPARK_HOME 
if [ ! -f "$SPARK_HOME/bin/spark-submit" ]; then
  echo "SPARK_HOME must be set to use sparkR-submit"
  exit 1
fi

source "$SPARK_HOME/bin/utils.sh"

function usage() {
  echo "Usage: ./sparkR-submit [options]" 1>&2
  "$SPARK_HOME"/bin/spark-submit --help 2>&1 | grep -v Usage 1>&2
  exit 0
}

if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then
  usage
fi


# Add SparkR to .libPaths
# If we are running an R program, only set libPaths and use Rscript

export R_PROFILE_USER="/tmp/sparkR.profile"

cat > /tmp/sparkR.profile << EOF
  .First <- function() {
  projecHome <- Sys.getenv("PROJECT_HOME")
  .libPaths(c(paste(projecHome,"/lib", sep=""), .libPaths()))
  Sys.setenv(NOAWT=1)
}
EOF

# Build up arguments list manually to preserve quotes and backslashes.
SUBMIT_USAGE_FUNCTION=usage
gatherSparkSubmitOpts "$@"

# If a R file is provided, directly run spark-submit. 
if [[ "${APPLICATION_OPTS[0]}" =~ \.R$ ]]; then
  primary="${APPLICATION_OPTS[0]}"
  shift
  # Set the main class to SparkRRunner and add the primary R file to --files to make sure its copied to the cluster
  exec "$SPARK_HOME"/bin/spark-submit --class edu.berkeley.cs.amplab.sparkr.SparkRRunner --files "$primary" "${SUBMISSION_OPTS[@]}" "$SPARKR_JAR_FILE" "$primary" "${APPLICATION_OPTS[@]:1}"
else
  echo "sparkR-submit can only be used to run R programs. Please use sparkR to launch a shell."
fi
