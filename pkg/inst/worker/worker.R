# Worker class

# Utility function to get current system time in milliseconds
# this code snippet borrowed from [R.utils](cran.r-project.org/package=R.utils)
currentTimeMillis <- function() {
  secs <- as.numeric(Sys.time())
  times <- proc.time()
  time <- times[2]  # System CPU time

  # CPU time is not available on Win 98/Me;
  if (is.na(time))
    time <- times[3] # Total elapsed times
  (secs + as.numeric(time) %% 1) * 1000
}

# Constants
SpecialLengths <- list(END_OF_STERAM = 0L, TIMING_DATA = -1L)

# Timing R process boot
bootTime <- currentTimeMillis()

port <- as.integer(Sys.getenv("SPARKR_WORKER_PORT"))

inputCon <- socketConnection(port = port, blocking = TRUE, open = "rb")
outputCon <- socketConnection(port = port, blocking = TRUE, open = "wb")

# Set libPaths to include SparkR package as loadNamespace needs this
# TODO: Figure out if we can avoid this by not loading any objects that require
# SparkR namespace
rLibDir <- readLines(inputCon, n = 1)
.libPaths(c(rLibDir, .libPaths()))

suppressPackageStartupMessages(library(SparkR))

# read the index of the current partition inside the RDD
splitIndex <- SparkR:::readInt(inputCon)

# read the isInputSerialized bit flag
isInputSerialized <- SparkR:::readInt(inputCon)

# read the isOutputSerialized bit flag
isOutputSerialized <- SparkR:::readInt(inputCon)

# Include packages as required
packageNames <- unserialize(SparkR:::readRaw(inputCon))
for (pkg in packageNames) {
  suppressPackageStartupMessages(require(as.character(pkg), character.only=TRUE))
}

# read function dependencies
funcLen <- SparkR:::readInt(inputCon)
computeFunc <- unserialize(SparkR:::readRawLen(inputCon, funcLen))
env <- environment(computeFunc)
parent.env(env) <- .GlobalEnv  # Attach under global environment.

# Timing init envs for computing
initTime <- currentTimeMillis()

# Read and set broadcast variables
numBroadcastVars <- SparkR:::readInt(inputCon)
if (numBroadcastVars > 0) {
  for (bcast in seq(1:numBroadcastVars)) {
    bcastId <- SparkR:::readInt(inputCon)
    value <- unserialize(SparkR:::readRaw(inputCon))
    setBroadcastValue(bcastId, value)
  }
}

# Timing broadcast
broadcastTime <- currentTimeMillis()

# If -1: read as normal RDD; if >= 0, treat as pairwise RDD and treat the int
# as number of partitions to create.
numPartitions <- SparkR:::readInt(inputCon)

isEmpty <- SparkR:::readInt(inputCon)

if (isEmpty != 0) {

  if (numPartitions == -1) {
    if (isInputSerialized) {
      # Now read as many characters as described in funcLen
      data <- SparkR:::readDeserialize(inputCon)
    } else {
      data <- readLines(inputCon)
    }
    # Timing reading input data for execution
    inputTime <- currentTimeMillis()

    output <- computeFunc(splitIndex, data)
    # Timing computing
    computeTime <- currentTimeMillis()

    if (isOutputSerialized) {
      SparkR:::writeRawSerialize(outputCon, output)
    } else {
      lapply(output,
             function(line) {
               # write lines one-by-one with flag
               SparkR:::writeString(outputCon, line)
             })
    }
    # Timing output
    outputTime <- currentTimeMillis()
  } else {
    if (isInputSerialized) {
      # Now read as many characters as described in funcLen
      data <- SparkR:::readDeserialize(inputCon)
    } else {
      data <- readLines(inputCon)
    }
    # Timing reading input data for execution
    inputTime <- currentTimeMillis()

    res <- new.env()

    # Step 1: hash the data to an environment
    hashTupleToEnvir <- function(tuple) {
      # NOTE: execFunction is the hash function here
      hashVal <- computeFunc(tuple[[1]])
      bucket <- as.character(hashVal %% numPartitions)
      acc <- res[[bucket]]
      # Create a new accumulator
      if (is.null(acc)) {
        acc <- SparkR:::initAccumulator()
      }
      SparkR:::addItemToAccumulator(acc, tuple)
      res[[bucket]] <- acc
    }
    invisible(lapply(data, hashTupleToEnvir))
    # Timing computing
    computeTime <- currentTimeMillis()

    # Step 2: write out all of the environment as key-value pairs.
    for (name in ls(res)) {
      SparkR:::writeInt(outputCon, 2L)
      SparkR:::writeInt(outputCon, as.integer(name))
      # Truncate the accumulator list to the number of elements we have
      length(res[[name]]$data) <- res[[name]]$counter
      SparkR:::writeRawSerialize(outputCon, res[[name]]$data)
    }
    # Timing output
    outputTime <- currentTimeMillis()
  }
} else {
  inputTime <- broadcastTime
  computeTime <- broadcastTime
  outputTime <- broadcastTime
}

# Timing finish
finishTime <- currentTimeMillis()

# Report timing
SparkR:::writeInt(outputCon, SpecialLengths$TIMING_DATA)
SparkR:::writeDouble(outputCon, bootTime)
SparkR:::writeDouble(outputCon, initTime)
SparkR:::writeDouble(outputCon, broadcastTime)
SparkR:::writeDouble(outputCon, inputTime)
SparkR:::writeDouble(outputCon, computeTime)
SparkR:::writeDouble(outputCon, outputTime)
SparkR:::writeDouble(outputCon, finishTime)

# End of output
SparkR:::writeInt(outputCon, SpecialLengths$END_OF_STERAM)

close(outputCon)
close(inputCon)
